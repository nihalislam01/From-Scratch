{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Linear Regression model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel:\n",
    "    def __init__(self):\n",
    "        #Holds the value of coefficients\n",
    "        self.theta = []\n",
    "        #Holds the value of predicted labels\n",
    "        self.predicted = []\n",
    "        #Number of rows the dataset has\n",
    "        self.rows = 0\n",
    "        #Learning rate\n",
    "        self.alpha = 0.01\n",
    "        #Actual Labels\n",
    "        self.label = []\n",
    "        #Actual Features (value of the data that will be given)\n",
    "        self.features = []\n",
    "        #Dimension of the input set\n",
    "        self.dimension = 0\n",
    "\n",
    "    def initialize_theta(self, method):\n",
    "        #Value of the Coefficients will be zero initially\n",
    "        if method == 'zeroinitialization': self.theta = [0 for _ in range(self.dimension + 1)]\n",
    "        #Value of the Coefficients will be random\n",
    "        elif method == 'randominitialization': self.theta = [random.uniform(-1, 1) for _ in range(self.dimension + 1)]\n",
    "        #Value of the Coefficients will be normalized random numbers\n",
    "        elif method == 'normalinitialization': self.theta = [random.gauss(0, 1) for _ in range(self.dimension + 1)]\n",
    "\n",
    "    #Splitting Labels from the dataset\n",
    "    def feature_label_split(self, dataset):\n",
    "        features, labels = zip(*[(data[:-1], data[-1]) for data in dataset])\n",
    "        self.features, self.label = list(features), list(labels)\n",
    "\n",
    "    #Linear Equation\n",
    "    def linear_function(self, row):\n",
    "        return sum(self.theta[index] * value for index, value in enumerate(row)) + self.theta[-1]\n",
    "\n",
    "    #Predicted value using the continuous coefficients (theta)\n",
    "    def predict(self, features):\n",
    "        self.predicted = [self.linear_function(row) for row in features]\n",
    "\n",
    "    #Mean Squared Error\n",
    "    def loss(self):\n",
    "        squared_errors = [(self.predicted[i] - self.label[i])**2 for i in range(self.rows)]\n",
    "        return sum(squared_errors) / (2 * self.rows)\n",
    "\n",
    "    #R-Squared\n",
    "    def r_squared(self):\n",
    "        mean_label = sum(self.label) / self.rows\n",
    "        total_variance = sum((y - mean_label) ** 2 for y in self.label)\n",
    "        explained_variance = sum((self.predicted[i] - self.label[i]) ** 2 for i in range(self.rows))\n",
    "        return 1 - (explained_variance / total_variance)\n",
    "\n",
    "    #Updating the value of Coefficients (theta)\n",
    "    def gradient_descent(self):\n",
    "        gradients = [0 for _ in range(self.dimension + 1)]\n",
    "\n",
    "        for i in range(self.rows):\n",
    "            error = self.predicted[i] - self.label[i]\n",
    "            for j in range(self.dimension):\n",
    "                gradients[j] += error * self.features[i][j]\n",
    "            gradients[-1] += error\n",
    "\n",
    "        gradients = [g / self.rows for g in gradients]\n",
    "\n",
    "        self.theta = [self.theta[i] - self.alpha * gradients[i] for i in range(self.dimension + 1)]\n",
    "\n",
    "    #Data training session\n",
    "    def fit(self, dataset, learning_rate=1, init_method='zeroinitialization', epochs=1):\n",
    "        self.rows = len(dataset)\n",
    "        self.alpha = learning_rate\n",
    "        self.dimension = len(dataset[0]) - 1\n",
    "        self.feature_label_split(dataset)\n",
    "        self.initialize_theta(init_method)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.predict(self.features)\n",
    "            self.gradient_descent()\n",
    "            current_rs = self.r_squared()\n",
    "            current_loss = self.loss()\n",
    "            print(f'Epoch {epoch + 1}/{epochs}: R-squared -> {current_rs:.4f}, Loss -> {current_loss:.4f}')\n",
    "\n",
    "    def get_predictions(self):\n",
    "        return self.predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Dataset\n",
    "alpha = 0.0001\n",
    "dataset = [(x1 := random.randint(0, 100), x2 := random.randint(0, 100), x1 + x2) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: R-squared -> -5.9902, Loss -> 5769.5260\n",
      "Epoch 2/10: R-squared -> -0.2505, Loss -> 1032.1170\n",
      "Epoch 3/10: R-squared -> 0.7763, Loss -> 184.6427\n",
      "Epoch 4/10: R-squared -> 0.9600, Loss -> 33.0371\n",
      "Epoch 5/10: R-squared -> 0.9928, Loss -> 5.9155\n",
      "Epoch 6/10: R-squared -> 0.9987, Loss -> 1.0628\n",
      "Epoch 7/10: R-squared -> 0.9998, Loss -> 0.1940\n",
      "Epoch 8/10: R-squared -> 1.0000, Loss -> 0.0380\n",
      "Epoch 9/10: R-squared -> 1.0000, Loss -> 0.0096\n",
      "Epoch 10/10: R-squared -> 1.0000, Loss -> 0.0041\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, learning_rate=alpha, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.24868185159994\n"
     ]
    }
   ],
   "source": [
    "model.predict([(245, 55)])\n",
    "print(model.get_predictions()[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT!\n",
    "Do we actually need ML to solve this problem?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1 (v3.12.1:2305ca5144, Dec  7 2023, 17:23:38) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
